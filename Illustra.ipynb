{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Illustra.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Illustra: Multi-text to Image\r\n",
        "\r\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent) // made by [eps696](https://github.com/eps696) [Vadim Epstein]  \r\n",
        "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly) for ideas\r\n",
        "\r\n",
        "## Features \r\n",
        "* **continuously processes phrase lists** (e.g. illustrating lyrics)\r\n",
        "* generates [FFT-encoded](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) image (massive detailed textures, a la deepdream)\r\n",
        "* fast convergence\r\n",
        "* undemanding for RAM - fullHD/4K and above\r\n",
        "* saving/loading FFT params to resume processing\r\n",
        "* can use both CLIP models at once (ViT and RN50)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\r\n",
        "\r\n",
        "Mark `resume` and upload `.pt` file, if you're resuming from the saved params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\r\n",
        "\r\n",
        "import subprocess\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "\r\n",
        "try: \r\n",
        "  !pip3 install googletrans==3.1.0a0\r\n",
        "  from googletrans import Translator, constants\r\n",
        "  translator = Translator()\r\n",
        "except: pass\r\n",
        "!pip install ftfy\r\n",
        "!pip install ssim # not needed, to avoid import error\r\n",
        "\r\n",
        "!apt-get -qq install ffmpeg\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/G', force_remount=True)\r\n",
        "gdir = !ls /G/\r\n",
        "gdir = '/G/%s/' % str(gdir[0])\r\n",
        "%cd $gdir\r\n",
        "work_dir = 'illustra'\r\n",
        "work_dir = gdir + work_dir + '/'\r\n",
        "import os\r\n",
        "os.makedirs(work_dir, exist_ok=True)\r\n",
        "%cd $work_dir\r\n",
        "\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "from math import exp\r\n",
        "import random\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "from skimage import exposure\r\n",
        "from base64 import b64encode\r\n",
        "import shutil\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "from IPython.display import HTML, Image, display, clear_output\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import ipywidgets as ipy\r\n",
        "# import glob\r\n",
        "from google.colab import output, files\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "!pip install git+https://github.com/openai/CLIP.git\r\n",
        "import clip\r\n",
        "model_vit, _ = clip.load('ViT-B/32')\r\n",
        "\r\n",
        "!git clone https://github.com/eps696/aphantasia\r\n",
        "%cd /content/aphantasia/\r\n",
        "from clip_fft import to_valid_rgb, fft_image, slice_imgs, checkout\r\n",
        "from utils import pad_up_to, basename, file_list, img_list, img_read\r\n",
        "from progress_bar import ProgressIPy as ProgressBar\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "resume = False #@param {type:\"boolean\"}\r\n",
        "if resume:\r\n",
        "  resumed = files.upload()\r\n",
        "  params_pt = list(resumed.values())[0]\r\n",
        "\r\n",
        "def makevid(seq_dir, size=None):\r\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\r\n",
        "  out_video = seq_dir + '.mp4'\r\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\r\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\r\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Upload text file\r\n",
        "\r\n",
        "translate = False #@param {type:\"boolean\"}\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "Set the desired video resolution and `duration` (in sec). \r\n",
        "\r\n",
        "Try setting `noise_scale` > 0 (maybe 4~8), if the images \"getting stuck\" with time.  \r\n",
        "Set `overscan` to produce semi-seamlessly tileable texture (when off, it's centered).  \r\n",
        "Turn on `dual_model` to optimize with both CLIP models at once (eats more RAM!).  \r\n",
        "Decrease `samples` if you face OOM for higher resolutions.  \r\n",
        "Increase `save_freq` for longer training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "duration =  120#@param {type:\"integer\"}\n",
        "#@markdown > Tweaks & tuning\n",
        "overscan = True #@param {type:\"boolean\"}\n",
        "noise_scale = 0. #@param {type:\"number\"}\n",
        "dual_model = False #@param {type:\"boolean\"}\n",
        "#@markdown > Training\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "learning_rate = .05\n",
        "fps = 25\n",
        "\n",
        "if dual_model is True:\n",
        "  print(' using dual-model optimization')\n",
        "  model_rn, _ = clip.load('RN50')\n",
        "  samples = samples // 2\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "text_file = list(uploaded)[0]\n",
        "texts = list(uploaded.values())[0].decode().split('\\n')\n",
        "texts = [tt.strip() for tt in texts if len(tt.strip())>0 and tt[0] != '#']\n",
        "print(' text file:', text_file)\n",
        "print(' total lines:', len(texts))\n",
        "\n",
        "workdir = os.path.join(work_dir, basename(text_file))\n",
        "steps = int(duration * fps * save_freq / len(texts))\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "params, image_f = fft_image(shape)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "optimizer = torch.optim.Adam(params, learning_rate)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "  \n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkout(num, tempdir, pbar):\n",
        "  with torch.no_grad():\n",
        "    img = image_f().cpu().numpy()[0]\n",
        "  save_img(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "  pbar.upd()\n",
        "\n",
        "def train(i, txt_enc, tempdir, pbar):\n",
        "  loss = 0\n",
        "  \n",
        "  noise = noise_scale * torch.randn(1, 1, *params[0].shape[2:4], 1).cuda() if noise_scale > 0 else 0.\n",
        "  img_out = image_f(noise)\n",
        "  # img_out = image_f()\n",
        "\n",
        "  imgs_sliced = slice_imgs([img_out], samples, norm_in, overscan=overscan, micro=None)\n",
        "  out_enc = model_vit.encode_image(imgs_sliced[-1])\n",
        "  if dual_model is True: # use both clip models\n",
        "      out_enc = torch.cat((out_enc, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "  loss -= 100*torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkout(i // save_freq, tempdir, pbar)\n",
        "\n",
        "def process(txt, num):\n",
        "  print(' ref text: ', txt)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    txt = translator.translate(txt, dest='en').text\n",
        "    print(' translated to:', txt)\n",
        "  tx = clip.tokenize(txt).cuda()\n",
        "  txt_enc = model_vit.encode_text(tx).detach().clone()\n",
        "  if dual_model is True:\n",
        "    txt_enc = torch.cat((txt_enc, model_rn.encode_text(tx).detach().clone()), 1)\n",
        "  out_name = '%02d-%s' % (num, txt.translate(str.maketrans(dict.fromkeys(list(\"\\n',—|!?/:;\\\\\"), \"\"))).replace(' ', '_').replace('\"', ''))\n",
        "  tempdir = os.path.join(workdir, out_name)\n",
        "  os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "  pbar = ProgressBar(steps // save_freq)\n",
        "  for i in range(steps):\n",
        "    train(i, txt_enc, tempdir, pbar)\n",
        "\n",
        "  # shutil.copy(img_list(tempdir)[-1], os.path.join(workdir, '%s-%d.jpg' % (out_name, steps)))\n",
        "  # os.system('ffmpeg -v warning -y -i %s\\%%03d.jpg \"%s.mp4\"' % (tempdir, os.path.join(workdir, out_name)))\n",
        "  HTML(makevid(tempdir))\n",
        "\n",
        "for i, txt in enumerate(texts):\n",
        "    process(txt, i)\n",
        "\n",
        "# %cd $work_dir\n",
        "# vid_list = ['file ' + v.replace('\\\\', '/') for v in file_list(workdir, 'mp4')]\n",
        "# with open('dir.txt', 'w') as ff:\n",
        "  # ff.write('\\n'.join(vid_list))\n",
        "# outname = basename(text_file) + '.mp4'\n",
        "# !ffmpeg -y -v warning -f concat -i dir.txt -c:v copy $outname\n",
        "# os.system('ffmpeg -y -v warning -f concat -i dir.txt -c:v copy %s.mp4' % basename(text_file))\n",
        "# os.remove('dir.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}