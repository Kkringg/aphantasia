{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text2Image_VQGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image tool\n",
        "\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + VQGAN from [Taming Transformers](https://github.com/CompVis/taming-transformers) // made by Vadim Epstein [[eps696](https://github.com/eps696)]  \n",
        "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly), [Hannu Toyryla](https://twitter.com/htoyryla) for ideas\n",
        "\n",
        "## Features \n",
        "* complex requests:\n",
        "  * image and/or text as main prompts  \n",
        "   (composition similarity controlled with [SSIM](https://github.com/Po-Hsun-Su/pytorch-ssim) loss)\n",
        "  * additional text prompt to subtract (suppress) topics\n",
        "  * criteria inversion (show \"the opposite\")\n",
        "\n",
        "* various CLIP models (including multi-language from [SBERT](https://sbert.net))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\n",
        "\n",
        "!pip install torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2 torchvision==0.8.2 ftfy==5.8 regex\n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from math import exp\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from base64 import b64encode\n",
        "import moviepy, moviepy.editor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install git+https://github.com/Po-Hsun-Su/pytorch-ssim\n",
        "import pytorch_ssim as ssim\n",
        "\n",
        "%cd /content\n",
        "!rm -rf aphantasia\n",
        "!git clone https://github.com/eps696/aphantasia\n",
        "%cd aphantasia/\n",
        "from clip_fft import slice_imgs, checkout\n",
        "from utils import pad_up_to, basename, img_list, img_read, plot_text\n",
        "from progress_bar import ProgressIPy as ProgressBar\n",
        "\n",
        "!pip install omegaconf>=2.0.0 # pytorch-lightning==1.0.8\n",
        "# !pip install PyYAML==5.3.1 torchtext==0.8.0 pytorch-lightning==1.2.2\n",
        "# !pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!mv taming-transformers/* ./\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "if not os.path.isdir('/content/models_TT'):\n",
        "  !mkdir -p /content/models_TT\n",
        "def getm(url, path):\n",
        "  if os.path.isfile(path) and os.stat(path).st_size > 0: \n",
        "    print(' already exists', path, os.stat(path).st_size)\n",
        "  else:\n",
        "    !wget $url -O $path\n",
        "getm('https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1', '/content/models_TT/last-1024.ckpt')\n",
        "getm('https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1', '/content/models_TT/model-1024.yaml')\n",
        "getm('https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1', '/content/models_TT/last-16384.ckpt')\n",
        "getm('https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1', '/content/models_TT/model-16384.yaml')\n",
        "\n",
        "workdir = '_out'\n",
        "tempdir = os.path.join(workdir, 'ttt')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# resume = False #@param {type:\"boolean\"}\n",
        "# if resume:\n",
        "#   resumed = files.upload()\n",
        "#   params_pt = list(resumed.values())[0]\n",
        "#   params_pt = torch.load(io.BytesIO(params_pt))\n",
        "\n",
        "def load_config(config_path, display=False):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if display:\n",
        "        print(yaml.dump(OmegaConf.to_container(config)))\n",
        "    return config\n",
        "\n",
        "def load_vqgan(config, ckpt_path=None):\n",
        "    model = VQModel(**config.model.params)\n",
        "    if ckpt_path is not None:\n",
        "        sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "    return model.eval()\n",
        "\n",
        "def vqgan_image(model, z):\n",
        "    x = model.decode(z)\n",
        "    x = (x+1.)/2.\n",
        "    return x\n",
        "\n",
        "class latents(torch.nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(latents, self).__init__()\n",
        "        init_rnd = torch.zeros(shape).normal_(0.,4.)\n",
        "        self.lats = torch.nn.Parameter(init_rnd.cuda())\n",
        "    def forward(self):\n",
        "        return self.lats # [1,256, h//16, w//16]\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  # out_sequence = seq_dir + '/%05d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  # !ffmpeg -y -v quiet -i $out_sequence $out_video\n",
        "  moviepy.editor.ImageSequenceClip(img_list(seq_dir), fps=25).write_videofile(out_video, verbose=False) # , ffmpeg_params=ffmpeg_params, logger=None\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "!nvidia-smi -L\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some `text` and/or upload some image to start.  \n",
        "Put to `subtract` the topics, which you would like to avoid in the result.  \n",
        "`invert` the whole criteria, if you want to see \"the totally opposite\".\n",
        "\n",
        "Options for non-English languages (use only one of them!):  \n",
        "`multilang` = use multi-language model, trained with ViT  \n",
        "`translate` = use Google translate (works with any visual model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Input\n",
        "\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "subtract = \"\" #@param {type:\"string\"}\n",
        "multilang = False #@param {type:\"boolean\"}\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "invert = False #@param {type:\"boolean\"}\n",
        "upload_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "  text = translator.translate(text, dest='en').text\n",
        "if upload_image:\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "### Settings\n",
        "\n",
        "Select CLIP visual `model` (results do vary!). I prefer ViT for consistency (and it's the only native multi-language option).  \n",
        "Select `VQGAN_size` - it also changes the result (I prefer codebook `1024`, despite it's smaller than `16384`).  \n",
        "`overscan` option produces more uniform composition (when off, it's more centered).  \n",
        "`sync` value adds SSIM loss between the output and input image (if there's one), allowing to \"redraw\" it with controlled similarity. \n",
        "\n",
        "Decrease `samples` if you face OOM.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "!rm -rf $tempdir\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "sideX = 800 #@param {type:\"integer\"}\n",
        "sideY = 600 #@param {type:\"integer\"}\n",
        "#@markdown > Config\n",
        "model = 'ViT-B/32' #@param ['ViT-B/32', 'RN101', 'RN50x4', 'RN50']\n",
        "VQGAN_size = 1024 #@param [1024, 16384]\n",
        "overscan = False #@param {type:\"boolean\"}\n",
        "sync =  0.3 #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 300 #@param {type:\"integer\"}\n",
        "samples = 32 #@param {type:\"integer\"}\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "\n",
        "# #@markdown > Tricks\n",
        "# no_text = 0.07 #@param {type:\"number\"}\n",
        "# enhance = 0. #@param {type:\"number\"}\n",
        "# diverse = -enhance\n",
        "# expand = abs(enhance)\n",
        "\n",
        "if multilang: model = 'ViT-B/32' # sbert model is trained with ViT\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  samples = int(samples * 0.75)\n",
        "print(' using %d samples' % samples)\n",
        "\n",
        "model_clip, _ = clip.load(model)\n",
        "modsize = 288 if model == 'RN50x4' else 224\n",
        "xmem = {'RN50':0.5, 'RN50x4':0.16, 'RN101':0.33}\n",
        "if 'RN' in model:\n",
        "  samples = int(samples * xmem[model])\n",
        "\n",
        "if multilang is True:\n",
        "    model_lang = SentenceTransformer('clip-ViT-B-32-multilingual-v1').cuda()\n",
        "\n",
        "def enc_text(txt):\n",
        "    if multilang is True:\n",
        "        emb = model_lang.encode([txt], convert_to_tensor=True, show_progress_bar=False)\n",
        "    else:\n",
        "        emb = model_clip.encode_text(clip.tokenize(txt).cuda())\n",
        "    return emb.detach().clone()\n",
        "\n",
        "# if diverse != 0:\n",
        "#  samples = int(samples * 0.5)\n",
        "        \n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "sign = 1. if invert is True else -1.\n",
        "\n",
        "if upload_image:\n",
        "  in_img = list(uploaded.values())[0]\n",
        "  print(' image:', list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(in_img).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()[:,:3,:,:]\n",
        "  in_sliced = slice_imgs([img_in], samples, modsize, transform=norm_in)[0]\n",
        "  img_enc = model_clip.encode_image(in_sliced).detach().clone()\n",
        "  if sync > 0:\n",
        "    overscan = True\n",
        "    ssim_loss = ssim.SSIM(window_size = 11)\n",
        "    ssim_size = [sideY//4, sideX//4]\n",
        "    img_in = F.interpolate(img_in, ssim_size).float()\n",
        "    # img_in = F.interpolate(img_in, (sideY, sideX)).float()\n",
        "  else:\n",
        "    del img_in\n",
        "  del in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 0:\n",
        "  print(' text:', text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  txt_enc = enc_text(text)\n",
        "#  if no_text > 0:\n",
        "#      txt_plot = torch.from_numpy(plot_text(text, modsize)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "#      txt_plot_enc = model_clip.encode_image(txt_plot).detach().clone()\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      subtract = translator.translate(subtract, dest='en').text\n",
        "      print(' translated to:', subtract) \n",
        "  txt_enc0 = enc_text(subtract)\n",
        "\n",
        "if multilang is True: del model_lang\n",
        "\n",
        "config_vqgan = load_config(\"/content/models_TT/model-%d.yaml\" % int(VQGAN_size), display=False)\n",
        "model_vqgan = load_vqgan(config_vqgan, ckpt_path=\"/content/models_TT/last-%d.ckpt\" % int(VQGAN_size)).cuda()\n",
        "\n",
        "shape = [1, 256, sideY//16, sideX//16]\n",
        "lats = latents(shape).cuda()\n",
        "optimizer = torch.optim.Adam(lats.parameters(), learning_rate)\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkout(num):\n",
        "  with torch.no_grad():\n",
        "    img = vqgan_image(model_vqgan, lats()).cpu().numpy()[0]\n",
        "  save_img(img, os.path.join(tempdir, '%04d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "prev_enc = 0\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = vqgan_image(model_vqgan, lats())\n",
        "\n",
        "  imgs_sliced = slice_imgs([img_out], samples, modsize, norm_in, overscan=overscan)\n",
        "  out_enc = model_clip.encode_image(imgs_sliced[-1])\n",
        "#  if diverse != 0:\n",
        "#    imgs_sliced = slice_imgs([vqgan_image(model_vqgan, lats())], samples, modsize, norm_in, overscan=overscan)\n",
        "#    out_enc2 = model_clip.encode_image(imgs_sliced[-1])\n",
        "#    loss += diverse * torch.cosine_similarity(out_enc, out_enc2, dim=-1).mean()\n",
        "#    del out_enc2; torch.cuda.empty_cache()\n",
        "  if upload_image:\n",
        "      loss += sign * 0.5 * torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "#      if no_text > 0:\n",
        "#          loss -= sign * no_text * torch.cosine_similarity(txt_plot_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * 0.5 * torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if sync > 0 and upload_image: # image composition sync\n",
        "      loss -= sync * ssim_loss(F.interpolate(img_out, ssim_size).float(), img_in)\n",
        "#  if expand > 0:\n",
        "#    global prev_enc\n",
        "#    if i > 0:\n",
        "#      loss += expand * torch.cosine_similarity(out_enc, prev_enc, dim=-1).mean()\n",
        "#    prev_enc = out_enc.detach()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkout(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "torch.save(lats.lats, tempdir + '.pt')\n",
        "files.download(tempdir + '.pt')\n",
        "files.download(tempdir + '.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}